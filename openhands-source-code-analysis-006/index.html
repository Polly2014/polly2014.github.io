<!doctype html><html><head><title>Polly Blog - AI Assistant, Tutorials, and Insights</title><meta content="Explore Polly Blog for AI tutorials, insights, and updates on cutting-edge technology." name=description><meta content="Polly, Blog, AI Blog, AI Assistant, Tutorials, Technology Blog, Baoli Wang" name=keywords><meta content="width=device-width,initial-scale=1" name=viewport><meta content="text/html; charset=utf-8" http-equiv=content-type><link href=https://cdn.jsdelivr.net/npm/purecss@3.0.0/build/pure-min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/purecss@3.0.0/build/grids-responsive-min.css rel=stylesheet><link href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css rel=stylesheet><link href=https://polly2014.github.io/css/style_new.css rel=stylesheet><script src="https://www.googletagmanager.com/gtag/js?id=G-8JD13N7PHS" async></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date());gtag('config','G-8JD13N7PHS')</script><body><div class=menu-toggle><img alt=Menu src=https://polly2014.github.io/images/polly.png></div><div class=overlay></div><div class="pure-g container"><div class="sidebar pure-u-1 pure-u-md-1-5"><div class=title><a class=pure-menu-heading href=https://polly2014.github.io> <img class="avatar pure-img-responsive" src=https://polly2014.github.io/images/polly.png> </a><div class=introduction><p>Polly's Blog</div><div class=nav><ul class=nav-links><li><a href=https://polly2014.github.io><i class="fas fa-home"></i>Home</a><li><a href=https://polly2014.github.io/archive><i class="fas fa-archive"></i>Archive</a><li><a href=https://polly2014.github.io/category><i class="fas fa-folder"></i>Category</a><li><a href=https://polly2014.github.io/blog><i class="fas fa-file-alt"></i>Posts</a><li><a href=https://polly2014.github.io/publication><i class="fas fa-file-pdf"></i>Research</a><li><a href=https://polly2014.github.io/changelog><i class="fas fa-history"></i>Change log</a><li><a href=https://polly2014.github.io/about><i class="fas fa-info-circle"></i>About Me</a></ul></div><div class=social><ul class=social-links><li><a href=mailto:26716201@qq.com><i class="fas fa-envelope"></i></a><li><a href=https://twitter.com/Polly__007><i class="fab fa-twitter"></i></a><li><a href=https://www.linkedin.com/in/baoliwang><i class="fab fa-linkedin-in"></i></a><li><a href=https://github.com/Polly2014><i class="fab fa-github"></i></a></ul></div></div></div><div class="content pure-u-1 pure-u-md-4-5"><div class=blog-post><h1>OpenHands 源码解析系列（六）：与大语言模型（LLM）的交互</h1><div class=content><p>在 OpenHands 中，大语言模型（LLM）是生成响应的核心组件之一。本文将深入解析系统如何通过 LLM 模块与大语言模型交互，帮助读者理解其实现细节和设计理念。<hr><h2 id=llm-mo-kuai-xiang-jie>LLM 模块详解</h2><p>LLM 模块的核心逻辑位于 <code>llm/llm.py</code>，以下是其主要功能的详细分析：<ol><li><p><strong>API 调用</strong>：</p> <ul><li><strong>功能</strong>：调用外部 LLM 服务（如 OpenAI GPT）。<li><strong>实现细节</strong>： <ul><li>使用 HTTP 请求与 LLM 服务交互。<li>支持多种模型（如 GPT-3.5、GPT-4）。</ul></ul><li><p><strong>流式响应</strong>：</p> <ul><li><strong>功能</strong>：支持流式生成响应，提升用户体验。<li><strong>实现细节</strong>： <ul><li>使用 WebSocket 或 HTTP/2 实现流式数据传输。<li>在前端逐步显示生成的内容。</ul></ul></ol><hr><h2 id=shi-li-dai-ma-xiang-jie>示例代码详解</h2><p>以下是一个完整的 LLM 调用示例，展示了如何与外部 LLM 服务交互：<pre class=language-python data-lang=python style=background:#2b303b;color:#c0c5ce><code class=language-python data-lang=python><span style=color:#b48ead>class </span><span style=color:#ebcb8b>LLM</span><span style=color:#eff1f5>:
</span><span>    </span><span style=color:#b48ead>def </span><span style=color:#96b5b4>__init__</span><span>(</span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>model_name</span><span>: str, </span><span style=color:#bf616a>api_key</span><span>: str):
</span><span>        </span><span style=color:#bf616a>self</span><span>.model_name = model_name
</span><span>        </span><span style=color:#bf616a>self</span><span>.api_key = api_key
</span><span>
</span><span>    </span><span style=color:#b48ead>def </span><span style=color:#8fa1b3>generate_response</span><span>(</span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>prompt</span><span>: str) -> str:
</span><span>        headers = {"</span><span style=color:#a3be8c>Authorization</span><span>": </span><span style=color:#b48ead>f</span><span>"</span><span style=color:#a3be8c>Bearer </span><span>{</span><span style=color:#bf616a>self</span><span>.api_key}"}
</span><span>        payload = {"</span><span style=color:#a3be8c>model</span><span>": </span><span style=color:#bf616a>self</span><span>.model_name, "</span><span style=color:#a3be8c>prompt</span><span>": prompt, "</span><span style=color:#a3be8c>max_tokens</span><span>": </span><span style=color:#d08770>100</span><span>}
</span><span>        response = requests.</span><span style=color:#bf616a>post</span><span>("</span><span style=color:#a3be8c>https://api.openai.com/v1/completions</span><span>", </span><span style=color:#bf616a>json</span><span>=payload, </span><span style=color:#bf616a>headers</span><span>=headers)
</span><span>        </span><span style=color:#b48ead>return </span><span>response.</span><span style=color:#bf616a>json</span><span>().</span><span style=color:#bf616a>get</span><span>("</span><span style=color:#a3be8c>choices</span><span>")[</span><span style=color:#d08770>0</span><span>].</span><span style=color:#bf616a>get</span><span>("</span><span style=color:#a3be8c>text</span><span>")
</span></code></pre><hr><h2 id=jiao-hu-liu-cheng-xiang-jie>交互流程详解</h2><p>以下是 OpenHands 与 LLM 的交互流程的详细分析：<ol><li><p><strong>生成提示</strong>：</p> <ul><li><strong>功能</strong>：代理根据用户输入生成提示（prompt）。<li><strong>实现细节</strong>： <ul><li>提示可以包含上下文信息，以提高生成结果的相关性。</ul></ul><li><p><strong>调用 LLM</strong>：</p> <ul><li><strong>功能</strong>：通过 <code>llm.py</code> 调用大语言模型。<li><strong>实现细节</strong>： <ul><li>使用 <code>generate_response</code> 方法发送请求并接收响应。</ul></ul><li><p><strong>返回响应</strong>：</p> <ul><li><strong>功能</strong>：LLM 返回生成的响应文本。<li><strong>实现细节</strong>： <ul><li>响应可以是完整的文本，也可以是流式数据。</ul></ul></ol><hr><h2 id=shen-du-fen-xi-kuo-zhan-xing-yu-you-hua>深度分析：扩展性与优化</h2><ol><li><p><strong>扩展性</strong>：</p> <ul><li>支持多种 LLM 服务： <ul><li>可以通过配置文件切换不同的 LLM 服务（如 OpenAI、Anthropic）。</ul><li>自定义提示模板： <ul><li>提供模板化的提示生成方式，适应不同的任务需求。</ul></ul><li><p><strong>性能优化</strong>：</p> <ul><li>缓存机制： <ul><li>对于常见的提示和响应结果进行缓存，减少重复调用。</ul><li>并发处理： <ul><li>使用异步编程（如 <code>asyncio</code>）同时处理多个 LLM 请求。</ul></ul></ol><hr><p>通过以上分析，我们可以看到 OpenHands 的 LLM 模块设计清晰且功能强大。在下一篇文章中，我们将解析事件流与存储管理的实现细节，带你了解其核心逻辑。<hr><h2 id=llm-mo-kuai>LLM 模块</h2><p>LLM 模块的核心逻辑位于 <code>llm/llm.py</code>，主要功能包括：<ol><li><p><strong>API 调用</strong>：</p> <ul><li>调用外部 LLM 服务（如 OpenAI GPT）。</ul><li><p><strong>流式响应</strong>：</p> <ul><li>支持流式生成响应，提升用户体验。</ul></ol><hr><h2 id=shi-li-dai-ma>示例代码</h2><p>以下是一个简单的 LLM 调用示例：<pre class=language-python data-lang=python style=background:#2b303b;color:#c0c5ce><code class=language-python data-lang=python><span style=color:#b48ead>class </span><span style=color:#ebcb8b>LLM</span><span style=color:#eff1f5>:
</span><span>    </span><span style=color:#b48ead>def </span><span style=color:#96b5b4>__init__</span><span>(</span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>model_name</span><span>: str):
</span><span>        </span><span style=color:#bf616a>self</span><span>.model_name = model_name
</span><span>
</span><span>    </span><span style=color:#b48ead>def </span><span style=color:#8fa1b3>generate_response</span><span>(</span><span style=color:#bf616a>self</span><span>, </span><span style=color:#bf616a>prompt</span><span>: str) -> str:
</span><span>        response = </span><span style=color:#bf616a>external_llm_api_call</span><span>(</span><span style=color:#bf616a>model</span><span>=</span><span style=color:#bf616a>self</span><span>.model_name, </span><span style=color:#bf616a>prompt</span><span>=prompt)
</span><span>        </span><span style=color:#b48ead>return </span><span>response
</span></code></pre><hr><h2 id=jiao-hu-liu-cheng>交互流程</h2><ol><li><p><strong>生成提示</strong>：</p> <ul><li>代理生成提示（prompt）。</ul><li><p><strong>调用 LLM</strong>：</p> <ul><li>通过 <code>llm.py</code> 调用大语言模型。</ul><li><p><strong>返回响应</strong>：</p> <ul><li>LLM 返回生成的响应文本。</ul></ol><hr><h2 id=zong-jie>总结</h2><p>通过与大语言模型的交互，OpenHands 能够生成高质量的响应。在下一篇文章中，我们将解析事件流与存储管理的实现细节。<hr><p>下一篇：<a href=#>OpenHands 源码解析系列（七）：事件流与存储管理</a></div><div class=navigation></div></div><div id=giscus-container><h2>留言与讨论</h2><div class=giscus></div></div><script data-category="Blog Comments" async crossorigin data-category-id=DIC_kwDOL45duM4CnjlZ data-emit-metadata=0 data-input-position=bottom data-lang=en data-mapping=pathname data-reactions-enabled=1 data-repo=Polly2014/polly2014.github.io data-repo-id=R_kgDOL45duA data-strict=0 data-theme=noborder_light src=https://giscus.app/client.js></script></div></div><script>document.addEventListener('DOMContentLoaded',function(){const c=document.querySelector('.menu-toggle');const d=document.querySelector('.sidebar');const e=document.querySelector('.overlay');function a(){d.classList.toggle('active');e.classList.toggle('active')}c.addEventListener('click',a);e.addEventListener('click',a);let f=0;let g=0;document.addEventListener('touchstart',h=>{f=h.changedTouches[0].screenX},false);document.addEventListener('touchend',h=>{g=h.changedTouches[0].screenX;b()},false);function b(){const h=g- f;if(h>50&&f<30){d.classList.add('active');e.classList.add('active')}else if(h<-50&&d.classList.contains('active')){d.classList.remove('active');e.classList.remove('active')}}})</script>