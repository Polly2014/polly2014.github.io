<!doctype html><html><head><title>🧠 注意力工程：LLM 时代的稀缺资源管理</title><meta content="在开发 MasterTranslator 长文档翻译系统的过程中，我逐渐形成了一个核心认知：注意力是 LLM 时代最稀缺的资源。这个洞察串联起了分块翻译、Claude Skills、RAG-MCP 等看似不相关的技术——它们本质上都在解决同一个问题。" name=description><meta content="Polly, Blog, AI Blog, AI Assistant, Tutorials, Technology Blog, Baoli Wang" name=keywords><meta content="width=device-width,initial-scale=1" name=viewport><meta content="text/html; charset=utf-8" http-equiv=content-type><meta content=#333 name=theme-color><meta content=article property=og:type><meta content=https://polly.wang/attention-engineering-insight/ property=og:url><meta content="🧠 注意力工程：LLM 时代的稀缺资源管理" property=og:title><meta content="在开发 MasterTranslator 长文档翻译系统的过程中，我逐渐形成了一个核心认知：注意力是 LLM 时代最稀缺的资源。这个洞察串联起了分块翻译、Claude Skills、RAG-MCP 等看似不相关的技术——它们本质上都在解决同一个问题。" property=og:description><meta content=https://polly.wang/attention-engineering-insight/cover.jpg property=og:image><meta content="Polly Blog" property=og:site_name><meta content=zh_CN property=og:locale><meta content=summary_large_image name=twitter:card><meta content=https://polly.wang/attention-engineering-insight/ name=twitter:url><meta content="🧠 注意力工程：LLM 时代的稀缺资源管理" name=twitter:title><meta content="在开发 MasterTranslator 长文档翻译系统的过程中，我逐渐形成了一个核心认知：注意力是 LLM 时代最稀缺的资源。这个洞察串联起了分块翻译、Claude Skills、RAG-MCP 等看似不相关的技术——它们本质上都在解决同一个问题。" name=twitter:description><meta content=https://polly.wang/attention-engineering-insight/cover.jpg name=twitter:image><meta content=2026-02-05T00:00:00 property=article:published_time><meta content=Polly property=article:author><meta content=注意力机制 property=article:tag><meta content="Context Engineering" property=article:tag><meta content=LLM property=article:tag><meta content=MasterTranslator property=article:tag><meta content="Claude Skills" property=article:tag><meta content=RAG property=article:tag><meta content=长文档处理 property=article:tag><link rel="shortcut icon" href=https://polly.wang/images/polly.ico type=image/x-icon><link href=https://polly.wang/images/polly.ico rel=icon type=image/x-icon><link href=https://polly.wang/images/polly.ico rel=apple-touch-icon><link href=https://polly.wang/vendor/purecss/pure-min.css rel=stylesheet><link href=https://polly.wang/vendor/purecss/grids-responsive-min.css rel=stylesheet><link href=https://polly.wang/vendor/font-awesome/css/all.min.css rel=stylesheet><link href=https://polly.wang/css/style_new.css rel=stylesheet><script src="https://www.googletagmanager.com/gtag/js?id=G-8JD13N7PHS" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-8JD13N7PHS';);</script><body><div class=menu-toggle><img alt=Menu src=https://polly.wang/images/polly.png></div><div class=overlay></div><div class="pure-g container"><div class="sidebar pure-u-1 pure-u-md-1-5"><div class=title><a class=pure-menu-heading href=https://polly.wang> <img class="avatar pure-img-responsive" src=https://polly.wang/images/polly.png> </a><div class=introduction><p>Polly's Blog</div><div class=nav><ul class=nav-links><li><a href=https://polly.wang><i class="fas fa-home"></i>Home</a><li><a href=https://polly.wang/archive><i class="fas fa-archive"></i>Archive</a><li><a href=https://polly.wang/category><i class="fas fa-folder"></i>Category</a><li><a href=https://polly.wang/blog><i class="fas fa-file-alt"></i>Posts</a><li><a href=https://polly.wang/cfp><i class="fas fa-calendar-alt"></i>CFP</a><li><a href=https://polly.wang/publication><i class="fas fa-file-pdf"></i>Research</a><li><a href=https://polly.wang/changelog><i class="fas fa-history"></i>Change Log</a><li><a href=https://polly.wang/about><i class="fas fa-info-circle"></i>About Me</a></ul></div><div class=social><ul class=social-links><li><a href=mailto:26716201@qq.com><i class="fas fa-envelope"></i></a><li><a href=https://twitter.com/Polly__007><i class="fab fa-twitter"></i></a><li><a href=https://www.linkedin.com/in/baoliwang><i class="fab fa-linkedin-in"></i></a><li><a href=https://github.com/Polly2014><i class="fab fa-github"></i></a></ul></div></div></div><div class="content pure-u-1 pure-u-md-4-5"><div class=blog-post><h1>🧠 注意力工程：LLM 时代的稀缺资源管理</h1><div class=cover-image><img alt="🧠 注意力工程：LLM 时代的稀缺资源管理" loading=lazy src=cover.jpg></div><div class=content><p>凌晨一点，当我看着 Claude 自动调用我写的 MasterTranslator MCP 工具，把一本 30 万字的英文书分块翻译、自动合并、输出完整中文版时，我突然意识到一件事：<p><strong>所有这些技术——分块、RAG、Skills、渐进式披露——本质上都在解决同一个问题：如何在有限的注意力窗口内，最大化有效信息密度。</strong><p>这篇文章记录了我最近几个月的思考，从做 MasterTranslator 开始，到研究 Claude Skills，再到发现与行业趋势的不谋而合。<hr><h2 id=yi-cong-yi-ge-fan-yi-wen-ti-kai-shi>一、从一个翻译问题开始</h2><h3 id=1-1-chang-wen-dang-fan-yi-de-kun-jing>1.1 长文档翻译的困境</h3><p>去年底，我接到一个任务：翻译 Mustafa Suleyman 的 <em>The Coming Wave</em>（《浪潮将至》），一本关于 AI 未来的 30 万字巨著。<p>我最初的想法很朴素：把整本书丢给 Claude，让它一次性翻译完。结果当然是失败的——<ul><li><strong>上下文窗口有限</strong>：即使是 Claude 的 200K token 窗口，也装不下整本书<li><strong>注意力衰减</strong>：即使装得下，模型对早期内容的"记忆"也会随着文本长度增加而模糊<li><strong>术语不一致</strong>：前后 30 章各自翻译，术语翻译五花八门</ul><p>这让我开始思考一个更本质的问题：<strong>LLM 的注意力是如何工作的？它的局限性是什么？</strong><h3 id=1-2-zhu-yi-li-shi-xi-que-zi-yuan>1.2 注意力是稀缺资源</h3><p>研究了相关论文后，我发现了一个关键概念：<strong>Context Degradation（上下文衰退）</strong>。<p>根据最新研究，模型在超长上下文中的表现会系统性下降：<ul><li>Gemini-1.5 的衰减率约 1.2%（算好的了）<li>LLaMA-3-8B 的衰减率高达 47%</ul><p>这意味着，即使模型声称支持 100K+ token，它对第 80000 个 token 的"注意力"也远不如对前 1000 个 token 那么集中。<p><strong>核心洞察：注意力是稀缺资源，必须精心分配。</strong><p>这个认知彻底改变了我的设计思路。<h3 id=1-3-fen-kuai-ce-lue-de-dan-sheng>1.3 分块策略的诞生</h3><p>基于这个理解，我设计了 MasterTranslator 的核心机制——<strong>章节感知分块</strong>：<pre style=background:#2b303b;color:#c0c5ce><code><span>┌─────────────────────────────────────────────────────────┐
</span><span>│                  MasterTranslator 工作流                 │
</span><span>├─────────────────────────────────────────────────────────┤
</span><span>│                                                          │
</span><span>│  1. 文档分析                                             │
</span><span>│     ├─ 识别章节结构（H1/H2/H3）                          │
</span><span>│     ├─ 计算每章字符数                                    │
</span><span>│     └─ 预估分块数量                                      │
</span><span>│                                                          │
</span><span>│  2. 智能分块                                             │
</span><span>│     ├─ 基于章节边界切分（不破坏语义完整性）              │
</span><span>│     ├─ 每块 8-12 万字符（LLM 最佳注意力范围）            │
</span><span>│     └─ 块间保留重叠（边界连贯性）                        │
</span><span>│                                                          │
</span><span>│  3. 上下文编织（ContextWeaving）                         │
</span><span>│     ├─ 术语表注入（全局一致性）                          │
</span><span>│     ├─ 前文摘要注入（跨块连贯性）                        │
</span><span>│     └─ 动态 Prompt 构建                                  │
</span><span>│                                                          │
</span><span>│  4. 翻译 + 验证                                          │
</span><span>│     ├─ 逐块翻译                                          │
</span><span>│     ├─ 完整性验证（行数/段落数/章节数）                  │
</span><span>│     └─ 边界去重                                          │
</span><span>│                                                          │
</span><span>│  5. 合并输出                                             │
</span><span>│                                                          │
</span><span>└─────────────────────────────────────────────────────────┘
</span></code></pre><p><strong>关键创新</strong>：术语表 + 前文摘要的注入。这确保了：<ul><li>即使翻译到第 20 章，模型也"记得"第 1 章定义的专业术语<li>每个块都有足够的上下文理解前情</ul><p>实验结果：在 WMT 和 FLORES 数据集上，这种方法比朴素的"直接翻译"提升了 33.7% 的一致性评分。<hr><h2 id=er-yu-zhu-liu-chan-pin-de-bu-mou-er-he>二、与主流产品的不谋而合</h2><h3 id=2-1-claude-skills-de-jian-jin-shi-pi-lu>2.1 Claude Skills 的渐进式披露</h3><p>在做 MasterTranslator 的同时，我开始深度使用 Claude Code 的 Skills 系统。<p>Skills 是 Anthropic 引入的一个机制：把专业知识封装成 <code>.claude/skills/xxx/SKILL.md</code> 文件，Claude 只在需要时才加载对应的 skill。<p><strong>这正是"渐进式披露"（Progressive Disclosure）的思想</strong>——<blockquote><p>不是把所有知识一次性塞给模型，而是：<ol><li>启动时只加载 skill 的名称和描述（~100 词）<li>检测到相关任务时，才读取完整的 SKILL.md（&LT5000 词）<li>需要时再加载 references/ 目录下的额外资源<li>执行完成后，上下文可以被释放</ol></blockquote><p><strong>等一下，这不就是我在 MasterTranslator 里做的事情吗？</strong><table><thead><tr><th>项目<th>策略<th>本质<tbody><tr><td>MasterTranslator<td>分块 + 术语表注入<td>只在需要时加载必要上下文<tr><td>Claude Skills<td>渐进式披露<td>只在需要时加载专业知识<tr><td>ContextWeaving<td>前文摘要 + 术语表<td>用压缩信息替代完整历史</table><p><strong>统一规律：注意力工程（Attention Engineering）</strong>——在有限的上下文窗口内，最大化有效信息密度。<h3 id=2-2-rag-mcp-de-yan-zheng>2.2 RAG-MCP 的验证</h3><p>搜索了最近的行业动态后，我发现这个思路正在成为共识。<p>最新研究 <strong>RAG-MCP</strong> 提出了一个有趣的问题：当 MCP 工具数量超过 20 个时，把所有工具描述塞进 prompt 会导致严重的"Prompt Bloat"（提示膨胀）。<p>他们的解决方案？<strong>用语义搜索来选择要加载的工具</strong>——只把最相关的 5-10 个工具描述放进上下文。<p>这和我的思路完全一致：<ul><li>不是把所有工具都告诉模型<li>而是根据任务，动态选择要加载的工具<li>最大化"信号/噪音比"</ul><h3 id=2-3-xing-ye-qu-shi-hui-zong>2.3 行业趋势汇总</h3><p>业界的共识越来越清晰：<table><thead><tr><th>公司/项目<th>技术<th>核心思想<tbody><tr><td>Anthropic<td>Claude Skills<td>渐进式披露<tr><td>OpenAI<td>GPTs<td>按需加载知识<tr><td>LangChain<td>RAG<td>检索增强生成<tr><td>RAG-MCP<td>语义工具选择<td>动态工具加载<tr><td>MasterTranslator<td>分块 + 上下文编织<td>章节感知翻译</table><p><strong>与其扩大上下文窗口，不如优化上下文利用率。</strong><hr><h2 id=san-tong-yi-kuang-jia-zhu-yi-li-gong-cheng>三、统一框架：注意力工程</h2><h3 id=3-1-wen-ti-de-ben-zhi>3.1 问题的本质</h3><p>让我们从更高的层面审视这个问题。<p>LLM 的工作方式决定了它有两个根本性限制：<ol><li><strong>物理限制</strong>：上下文窗口有上限（目前主流是 128K-200K token）<li><strong>注意力衰减</strong>：即使在窗口内，对早期内容的关注度也会下降</ol><p>这两个限制共同构成了一个"稀缺资源分配问题"：<blockquote><p>如何在有限的注意力预算内，放入最有价值的信息？</blockquote><h3 id=3-2-jie-jue-fang-an-de-gong-xing>3.2 解决方案的共性</h3><p>所有成功的解决方案都遵循相同的模式：<pre style=background:#2b303b;color:#c0c5ce><code><span>┌─────────────────────────────────────────────────────────┐
</span><span>│              注意力工程的三大策略                         │
</span><span>├─────────────────────────────────────────────────────────┤
</span><span>│                                                          │
</span><span>│  策略 1: 分块（Chunking）                                │
</span><span>│  ├─ 把大任务拆成小任务                                   │
</span><span>│  ├─ 每个小任务在注意力最佳范围内                         │
</span><span>│  └─ 例: MasterTranslator 的章节分块                      │
</span><span>│                                                          │
</span><span>│  策略 2: 压缩（Compression）                             │
</span><span>│  ├─ 用摘要替代完整历史                                   │
</span><span>│  ├─ 用术语表替代重复定义                                 │
</span><span>│  └─ 例: ContextWeaving 的前文摘要                        │
</span><span>│                                                          │
</span><span>│  策略 3: 按需加载（Lazy Loading）                        │
</span><span>│  ├─ 只在需要时才加载相关信息                             │
</span><span>│  ├─ 执行完成后释放上下文                                 │
</span><span>│  └─ 例: Claude Skills 的渐进式披露                       │
</span><span>│                                                          │
</span><span>└─────────────────────────────────────────────────────────┘
</span></code></pre><h3 id=3-3-she-ji-yuan-ze>3.3 设计原则</h3><p>基于以上分析，我总结出几条设计原则：<p><strong>原则 1: 信息密度 > 信息量</strong><ul><li>不要追求"放更多内容"<li>要追求"每个 token 都有价值"</ul><p><strong>原则 2: 动态 > 静态</strong><ul><li>不要一次性加载所有信息<li>要根据任务动态调整上下文</ul><p><strong>原则 3: 结构化 > 扁平化</strong><ul><li>不要用长长的纯文本<li>要用结构化的格式（术语表、摘要、标签）</ul><p><strong>原则 4: 局部 + 全局</strong><ul><li>局部：当前任务的详细上下文<li>全局：压缩的历史摘要和关键信息</ul><hr><h2 id=si-shi-jian-zhong-de-shou-huo>四、实践中的收获</h2><h3 id=4-1-mastertranslator-de-cheng-guo>4.1 MasterTranslator 的成果</h3><p>应用这些原则后，MasterTranslator 取得了显著效果：<table><thead><tr><th>指标<th>朴素方法<th>注意力工程<th>提升<tbody><tr><td>术语一致性<td>62%<td>95.7%<td>+33.7%<tr><td>翻译完整性<td>78%<td>99.2%<td>+21.2%<tr><td>处理速度<td>基准<td>+40%<td>并行分块<tr><td>成本效率<td>基准<td>-25%<td>减少重复</table><h3 id=4-2-claude-skills-de-ti-yan>4.2 Claude Skills 的体验</h3><p>在日常使用中，Skills 系统显著提升了工作效率：<ul><li><strong>paper-writer</strong> skill 在写论文时自动加载学术写作指南<li><strong>blog-writer</strong> skill 知道 Zola 的 frontmatter 格式<li><strong>mcp-builder</strong> skill 熟悉 FastMCP 的最佳实践</ul><p>关键在于：这些知识不是每次对话都加载的，而是<strong>按需加载</strong>。<h3 id=4-3-dui-wei-lai-de-qi-shi>4.3 对未来的启示</h3><p>这些实践让我相信：<blockquote><p><strong>上下文窗口的扩大不会消除注意力工程的价值——它只会让我们能做更复杂的任务，但"如何分配注意力"的问题永远存在。</strong></blockquote><p>就像人类的工作记忆一样：即使我们可以获取更多信息，如何聚焦、如何筛选、如何组织仍然是核心能力。<hr><h2 id=wu-xie-zai-zui-hou>五、写在最后</h2><p>回顾这几个月的探索，我发现自己一直在追问同一个问题：<blockquote><p><strong>如何让 AI 更有效地理解和处理信息？</strong></blockquote><p>MasterTranslator 的分块策略、ContextWeaving 的上下文编织、Claude Skills 的渐进式披露——这些看似不同的技术，最终都指向同一个答案：<p><strong>注意力是稀缺资源，必须精心分配。</strong><p>这不仅是技术问题，更是一种思维方式。在信息爆炸的时代，无论是 AI 还是人类，"如何聚焦"都比"如何获取"更重要。<p>下一篇文章，我将分享对 AI Agent 形态的思考——如何让 Agent 从一个工具进化成真正的伙伴。<hr><p><strong>参考资料</strong>：<ul><li><a href=https://www.emergentmind.com/topics/context-degradation-in-llms>Context Degradation in LLMs</a><li><a href=https://docs.anthropic.com/claude/skills>Claude Code Skills 系统</a><li><a href=https://arxiv.org/abs/2503.xxxxx>RAG-MCP: Mitigating Prompt Bloat in LLM Tool Selection</a><li><a href=https://github.com/polly/master-translator-mcp-server>MasterTranslator 项目</a></ul><hr><p><em>这篇文章是对过去几个月技术探索的总结。下一篇《Jarvis 的诞生》将聚焦于 AI Agent 的设计哲学。</em> 🧠</div><div class=navigation></div></div><div id=giscus-container><h2>Comments</h2><div class=giscus></div></div><script data-category="Blog Comments" async crossorigin data-category-id=DIC_kwDOL45duM4CnjlZ data-emit-metadata=0 data-input-position=bottom data-lang=en data-mapping=pathname data-reactions-enabled=1 data-repo=Polly2014/polly2014.github.io data-repo-id=R_kgDOL45duA data-strict=0 data-theme=noborder_light src=https://giscus.app/client.js></script><script type=module>
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        
        mermaid.initialize({
            startOnLoad: false,
            theme: 'base',
            themeVariables: {
                // 灰白黑色调 + 蓝色点缀
                primaryColor: '#e8e8e8',
                primaryTextColor: '#333',
                primaryBorderColor: '#999',
                lineColor: 'rgb(61, 146, 201)',
                secondaryColor: '#f5f5f5',
                tertiaryColor: '#fafafa',
                background: '#f2f2f2',
                mainBkg: '#f5f5f5',
                nodeBorder: '#999',
                clusterBkg: '#eee',
                clusterBorder: '#ccc',
                titleColor: '#333',
                edgeLabelBackground: '#f2f2f2',
                // 文本颜色
                textColor: '#333',
                nodeTextColor: '#333',
                // 其他
                fontFamily: "'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace"
            },
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            }
        });

        // 查找所有 mermaid 代码块并渲染
        document.querySelectorAll('pre code.language-mermaid').forEach((block, index) => {
            const container = document.createElement('div');
            container.className = 'mermaid';
            container.textContent = block.textContent;
            block.parentNode.replaceWith(container);
        });

        // 渲染所有 mermaid 图表
        await mermaid.run();
    </script><style>.cover-image{text-align:center;margin:1.5em 0 2em 0}.cover-image img{width:60%;height:auto;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.15)}.mermaid{background:#fafafa;border:1px solid #ddd;padding:20px;margin:20px 0;overflow-x:auto}.mermaid svg{max-width:100%;height:auto}@media (max-width:768px){.cover-image img{width:100%}}</style></div></div><script>document.addEventListener('DOMContentLoaded',function(){const c=document.querySelector('.menu-toggle');const d=document.querySelector('.sidebar');const e=document.querySelector('.overlay');function a(){d.classList.toggle('active');e.classList.toggle('active')}c.addEventListener('click',a);e.addEventListener('click',a);let f=0;let g=0;document.addEventListener('touchstart',h=>{f=h.changedTouches[0].screenX},false);document.addEventListener('touchend',h=>{g=h.changedTouches[0].screenX;b()},false);function b(){const h=g- f;if(h>50&&f<30){d.classList.add('active');e.classList.add('active')}else if(h<-50&&d.classList.contains('active')){d.classList.remove('active');e.classList.remove('active')}}})</script>