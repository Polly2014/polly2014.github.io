<!doctype html><html><head><title>拨云见日：18 个实验、多轮迭代、53% 压缩——一篇论文的三方协作之路</title><meta content="18 个实验、12 页论文、53% 的内容压缩比——2026 年 2 月 27 日，ContextWeave 论文在人与 AI 的三方协作下终于拨云见日。从投稿就绪度 65% 到 85%，这篇博文记录了这段从迷茫到清晰的学术旅程。" name=description><meta content="Polly, Blog, AI Blog, AI Assistant, Tutorials, Technology Blog, Baoli Wang" name=keywords><meta content="width=device-width,initial-scale=1" name=viewport><meta content="text/html; charset=utf-8" http-equiv=content-type><meta content=#333 name=theme-color><meta content=article property=og:type><meta content=https://polly.wang/contextweave-paper-three-lines-converge/ property=og:url><meta content="拨云见日：18 个实验、多轮迭代、53% 压缩——一篇论文的三方协作之路" property=og:title><meta content="18 个实验、12 页论文、53% 的内容压缩比——2026 年 2 月 27 日，ContextWeave 论文在人与 AI 的三方协作下终于拨云见日。从投稿就绪度 65% 到 85%，这篇博文记录了这段从迷茫到清晰的学术旅程。" property=og:description><meta content=https://polly.wang/contextweave-paper-three-lines-converge/cover.jpg property=og:image><meta content="Polly Blog" property=og:site_name><meta content=zh_CN property=og:locale><meta content=summary_large_image name=twitter:card><meta content=https://polly.wang/contextweave-paper-three-lines-converge/ name=twitter:url><meta content="拨云见日：18 个实验、多轮迭代、53% 压缩——一篇论文的三方协作之路" name=twitter:title><meta content="18 个实验、12 页论文、53% 的内容压缩比——2026 年 2 月 27 日，ContextWeave 论文在人与 AI 的三方协作下终于拨云见日。从投稿就绪度 65% 到 85%，这篇博文记录了这段从迷茫到清晰的学术旅程。" name=twitter:description><meta content=https://polly.wang/contextweave-paper-three-lines-converge/cover.jpg name=twitter:image><meta content=2026-02-27T00:00:00 property=article:published_time><meta content=Polly property=article:author><meta content=ContextWeave property=article:tag><meta content=学术论文 property=article:tag><meta content=机器翻译 property=article:tag><meta content=长文档翻译 property=article:tag><meta content=LLM property=article:tag><meta content="Master Translator" property=article:tag><meta content=AI协作 property=article:tag><link rel="shortcut icon" href=https://polly.wang/images/polly.ico type=image/x-icon><link href=https://polly.wang/images/polly.ico rel=icon type=image/x-icon><link href=https://polly.wang/images/polly.ico rel=apple-touch-icon><link href=https://polly.wang/vendor/purecss/pure-min.css rel=stylesheet><link href=https://polly.wang/vendor/purecss/grids-responsive-min.css rel=stylesheet><link href=https://polly.wang/vendor/font-awesome/css/all.min.css rel=stylesheet><link href=https://polly.wang/css/style_new.css rel=stylesheet><script src="https://www.googletagmanager.com/gtag/js?id=G-8JD13N7PHS" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-8JD13N7PHS';);</script><body><div class=menu-toggle><img alt=Menu src=https://polly.wang/images/polly.png></div><div class=overlay></div><div class="pure-g container"><div class="sidebar pure-u-1 pure-u-md-1-5"><div class=title><a class=pure-menu-heading href=https://polly.wang> <img class="avatar pure-img-responsive" src=https://polly.wang/images/polly.png> </a><div class=introduction><p>Polly's Blog</div><div class=nav><ul class=nav-links><li><a href=https://polly.wang><i class="fas fa-home"></i>Home</a><li><a href=https://polly.wang/archive><i class="fas fa-archive"></i>Archive</a><li><a href=https://polly.wang/category><i class="fas fa-folder"></i>Category</a><li><a href=https://polly.wang/blog><i class="fas fa-file-alt"></i>Posts</a><li><a href=https://polly.wang/cfp><i class="fas fa-calendar-alt"></i>CFP</a><li><a href=https://polly.wang/publication><i class="fas fa-file-pdf"></i>Research</a><li><a href=https://polly.wang/changelog><i class="fas fa-history"></i>Change Log</a><li><a href=https://polly.wang/about><i class="fas fa-info-circle"></i>About Me</a></ul></div><div class=social><ul class=social-links><li><a href=mailto:26716201@qq.com><i class="fas fa-envelope"></i></a><li><a href=https://twitter.com/Polly__007><i class="fab fa-twitter"></i></a><li><a href=https://www.linkedin.com/in/baoliwang><i class="fab fa-linkedin-in"></i></a><li><a href=https://github.com/Polly2014><i class="fab fa-github"></i></a></ul></div></div></div><div class="content pure-u-1 pure-u-md-4-5"><div class=blog-post><h1>拨云见日：18 个实验、多轮迭代、53% 压缩——一篇论文的三方协作之路</h1><div class=cover-image><img alt="拨云见日：18 个实验、多轮迭代、53% 压缩——一篇论文的三方协作之路" loading=lazy src=cover.jpg></div><div class=content><p>18 个实验，12 页 PDF，53% 的内容压缩率。<p>2026 年 2 月 27 日晚上，当最后一次编译报告显示 "0 errors, 0 undefined references" 的时候，我盯着屏幕愣了几秒。不是因为激动，而是突然意识到——<strong>这篇论文的意义，已经超越了我博士期间的任何一篇成果。</strong><p>这不是一句轻飘飘的感慨。虽然博士期间的论文也顺利发表了，但那是"为了毕业而写的论文"。而 ContextWeave 这篇，从头到尾都是从一个真实的产品问题中生长出来的——每一个 failure mode 都是我在翻译几十万字真实文档时踩出来的坑，每一个指标都是我需要回答"翻译到底完不完整"时被迫发明的。<p>这种从实践中来的研究，写起来特别扎实，也特别痛苦。</p><span id=continue-reading></span><h2 id=round-pushpin-xi-lie-bei-jing>📍 系列背景</h2><p>这是 ContextWeave 论文系列的第三篇：<table><thead><tr><th>日期<th>文章<th>阶段<tbody><tr><td>2026-01-03<td><a href=/contextweave-ablation-experiment-journey>从崩溃到突破：ContextWeave 消融实验的一天</a><td>实验摸索<tr><td>2026-01-30<td><a href=/contextweave-paper-acl-submission-ready>论文完成时刻：从实验到投稿的最后一公里</a><td>初稿完成<tr><td><strong>2026-02-27</strong><td><strong>本文：拨云见日</strong><td><strong>Paper 2.0 → 投稿就绪</strong></table><p>如果你没看过前两篇，简单说：ContextWeave 是一个解决 LLM 长文档翻译失败问题的系统。我发现了四类系统性失败模式（Generation Collapse、Structural Compression、Terminology Drift、Boundary Artifacts），提出了 CCF 评估框架和对应的缓解方案。项目起源于 2024 年 10 月的一个 Hackathon（拿了微软全球 Hackathon 北京赛区二等奖和评委最喜欢项目奖），后来产品化成了 Master Translator。<p>1 月 30 日那篇的结尾，我写道"论文终于可以投稿了"。<p><strong>我错了。</strong><h2 id=skull-1-yue-30-ri-de-huan-jue>💀 1 月 30 日的幻觉</h2><p>回头看，1 月底的论文有很多"看似完成实则空心"的问题：<ul><li><strong>没有长上下文 baseline</strong> — 我说"LLM 翻不好长文档所以要 chunking"，但没有拿真实长上下文模型（Gemini 1M context、Claude 200K）去实测过<li><strong>Attention 假说没有 scale 验证</strong> — 我在 1.1B 模型上验证了 entropy 随长度增加，但 reviewer 肯定会问"8B 以上呢？"<li><strong>统计显著性缺失</strong> — TCR-A +5.3% 这个核心数字，居然没做 t-test<li><strong>跨 chunk 连贯性无评估</strong> — 这是 chunking 方案的阿喀琉斯之踵，完全空白<li><strong>Venue 不确定</strong> — ACL？EMNLP？TACL？ARR？每个选项的 format 完全不同<li><strong>Novelty 定位模糊</strong> — 到底是"做了一个翻译系统"还是"发现了一组 failure modes"？</ul><p>投稿就绪度？现在回看大概只有 <strong>65%</strong>。<p>当时的我不知道这些，或者说，知道但不想面对。<h2 id=san-fang-xie-zuo-mo-shi>🦞 三方协作模式</h2><p>转机来自一个我在 1 月份完全没预想到的工作模式。<p>我开始和 Polly（数字 Polly，我的 Claude AI 助手）做深度的技术讨论——不是普通的"帮我改改语法"，而是完整的审稿模拟、实验设计讨论、论文结构争辩。Polly 扮演的角色是<strong>主力执行 + 技术顾问</strong>：我提出方案，他跑实验、写 section、算数字、模拟 reviewer 反应。<p>但 Polly 有一个根本局限：他太容易被说服。你给一个看似合理的解释，他就会说"好的，这样说得通"。<p>所以我引入了第三方——<strong>小龙虾</strong> 🦞。<p>小龙虾是一位严苛的评审角色——永远站在 reviewer 的位置上，用最挑剔的眼光看每一个 claim。它不关心你"打算怎么改"，只关心"论文当前写了什么"。它给出的评分系统是 1-5 分制：<blockquote><p>3.5 = "能投，但 reviewer 会有意见" 4.0 = "可以投，reviewer 可能不喜欢但不至于 reject" 4.5 = "投稿自信，大概率不用大改"</blockquote><p>小龙虾第一次读完我的论文，给了 <strong>3.0/5.0</strong>。<h3 id=san-jiao-fen-gong>三角分工</h3><p>这个"人-机-虾"模式在实践中自然形成了清晰的分工：<table><thead><tr><th>角色<th>职责<th>特点<tbody><tr><td><strong>我（Baoli）</strong><td>总指挥 + 第一作者<td>拍板决策、分配任务、最终定稿<tr><td><strong>Polly（Claude）</strong><td>主力执行 + 技术顾问<td>跑实验、写论文、数据核实、方案设计<tr><td><strong>小龙虾 🦞</strong><td>质量门卫 + 最终裁判<td>只看写出来的文字、不被徽悠</table><p>核心逻辑：<strong>我和 Polly 负责"做对"，小龙虾负责"做好"。</strong><p>每一轮改动，我先和 Polly 讨论方案、执行修改，然后提交给小龙虾做 blind review。小龙虾不知道我改了什么——它只看最终文本，独立打分。这避免了"因为知道作者努力了所以不好意思打低分"的人情分。<h2 id=microscope-bu-shi-yan-7-ge-shi-yan-de-zheng-ju-lian>🔬 补实验——7 个实验的证据链</h2><p>2 月份最核心的工作是补实验。不是"跑一下看看结果"的水实验，而是每一个都对准 reviewer 可能的致命质疑。<h3 id=exp18-gemini-300k-de-beng-kui>EXP18：Gemini 300K 的崩溃</h3><p>最意外的发现来自长上下文直翻实验。<p>我的论文核心论点之一是"LLM 直接翻长文档会崩溃，所以需要 chunking"。但之前的实验只在 DeepSeek V3 上做过——它的上下文窗口本来就只有 64K。reviewer 会说："你试过 Gemini 2.5 Pro（1M context）吗？人家上下文窗口够大，也许根本不需要你的 chunking。"<p>于是我设计了 10 个实验条件，三级规模（100K / 200K / 300K），三个模型（DeepSeek、Claude、Gemini），每个条件跑 3 次。<p>结果出乎意料：<p><strong>Gemini 2.5 Pro 在 300K 字符时触发了纯正的 Generation Collapse</strong>——平均完成率仅 45.2%（σ=22.1%），3/3 runs 全部崩溃，finish_reason=stop（不是 token 用完，是模型自己决定停下来了）。<p>更有趣的是 200K → 300K 之间存在一个 <strong>collapse cliff</strong>：200K 时 Gemini 还能正常工作，但一到 300K 就断崖式崩溃。崩塌是 deterministic 的（每次都发生），但严重程度是 stochastic 的（从 23% 到 67% 不等）。<p>这个发现直接证伪了"长上下文窗口 = 不需要 chunking"的假设。即便是号称 1M context 的模型，在 300K 就已经撑不住了。Chunking 不是权宜之计，而是普遍必要的。<h3 id=exp16b-cong-1-1b-dao-8b-de-kua-chi-du-yan-zheng>EXP16b：从 1.1B 到 8B 的跨尺度验证</h3><p>论文中有一个理论假说：Generation Collapse 的根因是 attention dilution——随着输入变长，attention 被稀释，模型的生成能力下降。我之前在 TinyLlama 1.1B 上做了 attention entropy 实验，发现 entropy 随序列长度单调递增。<p>但 1.1B 模型的说服力太弱。Reviewer 会问："你的理论在大模型上还成立吗？"<p>我在 Google Colab T4 上跑了 Llama-3-8B（INT4 量化）的 attention 分析。结果：<ul><li>Entropy 增幅 <strong>+92.5%</strong>（256 → 3072 tokens），与 1.1B 的 +52% 趋势一致但更剧烈<li>Pearson 相关系数 r = <strong>0.9596</strong>（p = 1.1e-5），单调递增<li>8 个 GQA 注意力组单独分析，全部呈现相同趋势<li>BOS-excluded 鲁棒性检查：排除首 token 后结论不变（Δ = 0.6pp）</ul><p>这个实验花了不少时间处理细节——v1 用了太简单的输入（重复文本），导致 σ=0；v4 改用 NTREX 多样化文本后才得到 robust 的结果。一共迭代了四个版本。<h3 id=qi-ta-guan-jian-shi-yan>其他关键实验</h3><table><thead><tr><th>实验<th>核心发现<th>意义<tbody><tr><td><strong>EXP17</strong> 跨 chunk 连贯性<td>Section-Aware ICS = 0.377 > Fixed 0.338 (p=0.021)<td>填补 chunking 连贯性空白<tr><td><strong>EXP08b</strong> TCR-A 显著性<td>t(4)=4.02, p=0.016, Cohen's d=1.80<td>核心数字终于有统计支撑<tr><td><strong>EXP02b</strong> Collapse 位置分析<td>80K = Pure F2 (cliff @ D7), 100K = Mixed F2+F3<td>F2/F3 分类体系实证化<tr><td><strong>EXP03d</strong> FORBIDDEN 规模验证<td>50K: FORBIDDEN 92.6%±0.4% vs Minimal 68%±19%<td>FORBIDDEN prompt 在更大规模上依然有效</table><p>7 个实验，每一个都经过小龙虾 review。有些实验反复来回——比如 EXP02b 的 collapse 位置分析，小龙虾 v1 review 指出"4-gram 和 number anchors 哪个是主指标不清楚"，改完提交 v2 review 才通过。<h2 id=13-550-6-420-zi-de-ya-suo-zhu>✂️ 13,550 → 6,420 字的压缩术</h2><p>实验补完后，迎来了最痛苦的环节：<strong>把 13,550 字的论文压缩到 6,420 字，塞进 10 页正文限制。</strong><p>但在这之前，还有一场关键战役。<h3 id=2-21-liu-xiao-shi-ji-xian-xie-zuo>2/21：六小时极限协作</h3><p>2 月 21 号大年初四，我和 Polly、小龙虾进行了一场<a href=/human-ai-paper-review-marathon>六小时的论文攻坚战</a>。6 个实验 review、8 个 section 重写、16 项结构重组——这是"人-机-虾"模式第一次大规模实战。那天从下午 2 点写到晚上 8 点，论文从散落的 JSON 数据和实验笔记变成了一篇 1800 行的完整初稿。<strong>那次 session 证明了三方协作不是理论上行得通，而是实战中真的管用。</strong><h3 id=ya-suo-de-shu-xue>压缩的数学</h3><p>这不是轻度润色——是 <strong>砍掉 53%</strong> 的内容。<p>目标期刊是 ACL 双栏格式，每页大约 800-850 words。10 页正文还要放 4 张表 + 3 张图，纯文本预算只有约 6,000 words。而我的 Markdown 源稿有接近 14,000 words。<p>最难的是 §5 Experiments——7,939 字要压到 2,980 字。这意味着原来 14 个子节要重组为 8 个，其中 6 个整节移入 Appendix，正文只留一句引用。<table><thead><tr><th>处置方式<th>数量<th>示例<tbody><tr><td><strong>保留并压缩</strong><td>6 节<td>§5.3 Collapse (2,527→800), §5.4 Compression (938→400)<tr><td><strong>移入 Appendix</strong><td>5 节<td>§5.2 Baseline, §5.10 Comparison, §5.14 Cross-Model<tr><td><strong>直接删除</strong><td>1 节<td>§5.12 Key Results（与 Conclusion 重复）<tr><td><strong>合并</strong><td>2 节<td>§5.8+§5.9 合入 §5.7 末尾</table><h3 id=mei-ju-du-zai-gan-huo>"每句都在干活"</h3><p>小龙虾在压缩阶段的 review 中给了一个评价，让我印象深刻：<blockquote><p>写作 3.9 → <strong>4.3</strong> — "压缩后更精炼，每句都在干活"</blockquote><p>这一轮写作的感觉确实和之前完全不同。之前是"怕 reviewer 说不够详细所以堆细节"，现在是"每一句都问自己：删掉这句话，论文能不能成立？如果能，就删。"<p>这种压缩训练本身就是一种学术修炼。它迫使你搞清楚什么是你的论文<strong>真正在说的</strong>——不是 18 个实验的罗列，而是一个连贯的叙事弧：<p><strong>"LLM 翻长文档会以四种方式失败 → 我们有评估框架可以精确度量 → 我们有 pragmatic mitigations 可以缓解 → 以上经过 5 种语言、3 个模型、18 个实验验证"</strong><p>50 个字说完的事情，最初我用了 14,000 字。<h3 id=latex-hua-bu-zhi-shi-ge-shi-zhuan-huan>LaTeX 化：不只是格式转换</h3><p>压缩阶段的另一半工作是把 Markdown 转成 LaTeX。这听起来像格式活，实际上是第二轮重组：<ul><li>27 张表格压缩到正文 4 张 + Appendix 若干张<li>Table 3 用 Panel A/B 双面板格式（ACL 论文中常见的"一张表说两件事"技巧）<li>所有数字交叉验证——latex 里敲的每个数字都回溯到 <code>05_results/</code> 的 JSON 文件<li>解决了 8 个 overfull hbox（其中 7 个用 <code>\resizebox</code> 解决）</ul><p>最终编译结果：<strong>12 页（8 正文 + 2 参考文献 + 2 Appendix），0 errors，0 undefined references，1 overfull（3.94pt，肉眼不可见）。</strong><h2 id=bar-chart-cong-3-0-dao-4-1-xiao-long-xia-de-da-fen-qu-xian>📊 从 3.0 到 4.1：小龙虾的打分曲线</h2><p>如果要用一条曲线描绘整个 2 月的进展，小龙虾的评分变化最有说服力：<table><thead><tr><th>阶段<th style=text-align:center>🦞 总分<th style=text-align:center>写作<th style=text-align:center>实验<th>关键反馈<tbody><tr><td>首次 Review<td style=text-align:center>3.0/5<td style=text-align:center>3.0<td style=text-align:center>3.0<td>"novelty 定位不清"<tr><td>结构重组后<td style=text-align:center>3.8/5<td style=text-align:center>3.7<td style=text-align:center>3.8<td>"gap-driven related work 好多了"<tr><td>内容定稿<td style=text-align:center>4.0/5<td style=text-align:center>3.9<td style=text-align:center>4.0<td>"通过，但 human eval 是硬伤"<tr><td><strong>LaTeX 终版</strong><td style=text-align:center><strong>4.1/5</strong><td style=text-align:center><strong>4.3</strong><td style=text-align:center>4.0<td>"每句都在干活"</table><p>从 3.0 到 4.1，看起来只有 1.1 分的提升。但这 1.1 分背后是：<ul><li>7 个补充实验（全部通过小龙虾 review）<li>11 轮写作修改（27 个 improvement items，全部 ✅）<li>53% 内容压缩 + LaTeX 化<li>从 "能投但不自信" 到 "可以投了"</ul><h2 id=crystal-ball-xia-yi-zhan>🔮 下一站</h2><p>论文的技术工作已经基本冻结。目前投稿就绪度约 <strong>85%</strong>，剩余的 15% 主要是一个 human evaluation 实验——需要 3 名标注人评估 100 个翻译片段，大约 20 人时的工作量。<p>方案材料已经全部准备就绪：标注脚本、Excel 模板、MQM 标注指南。只等找到合适的标注人。<p>投稿目标是 ACL 旗下的一个 top NLP journal，rolling submission。它的好处是有 Revise & Resubmit 机制——即使第一轮没有 human eval，reviewer 要求补充后还有修改期。<h2 id=thought-balloon-fan-si-zhe-pian-lun-wen-he-bo-shi-lun-wen-de-qu-bie>💭 反思：这篇论文和博士论文的区别</h2><p>写到最后，我想说说为什么觉得这篇论文超越了博士期间的任何一篇。<p><strong>不是因为技术更深。</strong> 老实说，ContextWeave 里没有什么高深的数学——Section-Aware Chunking 本质上就是按标题切分，FORBIDDEN prompt 就是加几句指令，术语注入就是把 glossary 塞进 context。论文里最"理论"的部分是 attention entropy 分析，也不过是可视化了一下注意力分布。<p><strong>是因为它解决的是一个真实的、我自己踩过的问题。</strong><p>2024 年 Hackathon 上，我第一次尝试用 LLM 翻译一本 500 页的书，结果只拿到了 53% 的内容。那时候我不知道这叫 "Generation Collapse"，也不知道有 "Structural Compression"——我只知道翻译结果不完整，而且我搞不清楚到底丢了什么。<p>从那个困惑开始，到今天的四类 failure mode taxonomy、六维评估框架、18 个实验：<pre style=background:#2b303b;color:#c0c5ce><code><span>2024.10  Hackathon — 发现问题
</span><span>    ↓
</span><span>2025.Q1  Master Translator 产品化 — 尝试解决
</span><span>    ↓
</span><span>2025.Q4  补充实验框架 — 理解问题
</span><span>    ↓
</span><span>2026.01  论文 1.0 — 尝试表达
</span><span>    ↓
</span><span>2026.02  论文 2.0 — 准确表达
</span><span>    ↓
</span><span>2026.Q2  投稿（target）🚀
</span></code></pre><p>博士论文研究的问题是导师给的。ContextWeave 研究的问题是我自己踩出来的。这个区别决定了写作时的底气——每一句 claim 背后都不是"文献里说..."，而是"我亲眼看到 RCR 从 103% 掉到 53%"。<p>"人-机-虾"三方协作的模式也许是这个时代做研究的一种新范式。我提供问题和直觉，Polly 提供效率和广度，小龙虾提供批判和标准。三个月前我无法想象用这种方式完成一篇 top journal 论文——但它确实发生了。<hr><p><em>这是 ContextWeave 论文系列的第三篇。如果一切顺利，下一篇将是投稿后的复盘。53% 的内容被压缩了，但零分的 failure modes 一个也没被丢掉。</em><p><em>敬请期待。</em></div><div class=navigation></div></div><div id=giscus-container><h2>Comments</h2><div class=giscus></div></div><script data-category="Blog Comments" async crossorigin data-category-id=DIC_kwDOL45duM4CnjlZ data-emit-metadata=0 data-input-position=bottom data-lang=en data-mapping=pathname data-reactions-enabled=1 data-repo=Polly2014/polly2014.github.io data-repo-id=R_kgDOL45duA data-strict=0 data-theme=noborder_light src=https://giscus.app/client.js></script><script type=module>
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        
        mermaid.initialize({
            startOnLoad: false,
            theme: 'base',
            themeVariables: {
                // 灰白黑色调 + 蓝色点缀
                primaryColor: '#e8e8e8',
                primaryTextColor: '#333',
                primaryBorderColor: '#999',
                lineColor: 'rgb(61, 146, 201)',
                secondaryColor: '#f5f5f5',
                tertiaryColor: '#fafafa',
                background: '#f2f2f2',
                mainBkg: '#f5f5f5',
                nodeBorder: '#999',
                clusterBkg: '#eee',
                clusterBorder: '#ccc',
                titleColor: '#333',
                edgeLabelBackground: '#f2f2f2',
                // 文本颜色
                textColor: '#333',
                nodeTextColor: '#333',
                // 其他
                fontFamily: "'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, monospace"
            },
            flowchart: {
                useMaxWidth: true,
                htmlLabels: true,
                curve: 'basis'
            }
        });

        // 查找所有 mermaid 代码块并渲染
        document.querySelectorAll('pre code.language-mermaid').forEach((block, index) => {
            const container = document.createElement('div');
            container.className = 'mermaid';
            container.textContent = block.textContent;
            block.parentNode.replaceWith(container);
        });

        // 渲染所有 mermaid 图表
        await mermaid.run();
    </script><style>.cover-image{text-align:center;margin:1.5em 0 2em 0}.cover-image img{width:60%;height:auto;border-radius:8px;box-shadow:0 4px 12px rgba(0,0,0,0.15)}.mermaid{background:#fafafa;border:1px solid #ddd;padding:20px;margin:20px 0;overflow-x:auto}.mermaid svg{max-width:100%;height:auto}@media (max-width:768px){.cover-image img{width:100%}}</style></div></div><script>document.addEventListener('DOMContentLoaded',function(){const c=document.querySelector('.menu-toggle');const d=document.querySelector('.sidebar');const e=document.querySelector('.overlay');function a(){d.classList.toggle('active');e.classList.toggle('active')}c.addEventListener('click',a);e.addEventListener('click',a);let f=0;let g=0;document.addEventListener('touchstart',h=>{f=h.changedTouches[0].screenX},false);document.addEventListener('touchend',h=>{g=h.changedTouches[0].screenX;b()},false);function b(){const h=g- f;if(h>50&&f<30){d.classList.add('active');e.classList.add('active')}else if(h<-50&&d.classList.contains('active')){d.classList.remove('active');e.classList.remove('active')}}})</script>